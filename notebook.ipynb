{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91e7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de445766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m4\u001b[0m)    │            \u001b[38;5;34m40\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m1,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │        \u001b[38;5;34m65,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m34\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,090</span> (262.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,090\u001b[0m (262.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,090</span> (262.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,090\u001b[0m (262.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a lightweight CNN model for audio classification.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input spectrograms (height, width, channels).\n",
    "        num_classes (int): The number of output classes (e.g., 2 for melody vs. other).\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input Layer\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        # First Convolutional Block\n",
    "        # Using smaller filters (3x3) and fewer of them (8) to keep the model small.\n",
    "        layers.Conv2D(4, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Flatten the feature map to feed into the dense layers\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Dense Layer for classification\n",
    "        # A smaller dense layer (32 units) to reduce parameters\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.5), # Dropout helps prevent overfitting\n",
    "\n",
    "        # Output Layer\n",
    "        # The number of units equals the number of classes.\n",
    "        # Use 'softmax' for multi-class or 'sigmoid' for binary classification.\n",
    "        layers.Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example of how to create the model\n",
    "# These values will be determined during data preprocessing\n",
    "INPUT_SHAPE = (128, 128, 1) # (n_mels, time_steps, channels)\n",
    "NUM_CLASSES = 2 # (lg_melody, other_sounds)\n",
    "\n",
    "model = build_cnn_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46aef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/karoldvl/ESC-50/archive/master.zip\n",
      "645701632/Unknown \u001b[1m44s\u001b[0m 0us/step"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./datasets/esc-50_extracted'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.get_file('esc-50.zip',\n",
    "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
    "                        cache_dir='./',\n",
    "                        extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e18f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'other_sounds': 0, 'lg_melody': 1}\n",
      "Training set shape: (2037, 64, 48, 1)\n",
      "Validation set shape: (113, 64, 48, 1)\n",
      "Test set shape: (114, 64, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"sample_rate\": 16000,    # Hz\n",
    "    \"window_duration\": 1.5,  # seconds (length of one spectrogram)\n",
    "    \"hop_duration\": 0.1,     # seconds (how much to slide the window)\n",
    "    \"n_mels\": 64,            # Number of Mel bands (reduced for ESP32 efficiency)\n",
    "    \"n_fft\": 1024,           # Number of FFT points\n",
    "    \"max_spectrogram_width\": 48 # Fixed width for spectrograms (time steps)\n",
    "}\n",
    "\n",
    "\n",
    "def process_audio_file(audio_path, class_label, config):\n",
    "    \"\"\"\n",
    "    Loads an audio file and converts it into one or more Mel spectrograms.\n",
    "\n",
    "    - If class_label is 'lg_melody', it uses a sliding window to generate\n",
    "      multiple, overlapping spectrograms from the entire clip.\n",
    "    - Otherwise, it generates a single spectrogram from the start of the clip.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        class_label (str): The name of the class (e.g., 'lg_melody').\n",
    "        config (dict): A dictionary of processing parameters.\n",
    "\n",
    "    Returns:\n",
    "        A list of spectrograms. Returns an empty list if processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=config[\"sample_rate\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    spectrograms = []\n",
    "    window_samples = int(config[\"window_duration\"] * config[\"sample_rate\"])\n",
    "    hop_samples = int(config[\"hop_duration\"] * config[\"sample_rate\"])\n",
    "\n",
    "    if class_label == 'lg_melody':\n",
    "        # --- Sliding Window for the Target Melody ---\n",
    "        for start in range(0, len(y) - window_samples, hop_samples):\n",
    "            end = start + window_samples\n",
    "            chunk = y[start:end]\n",
    "            \n",
    "            # Generate Mel spectrogram for the chunk\n",
    "            spectrogram = librosa.feature.melspectrogram(\n",
    "                y=chunk, \n",
    "                sr=sr, \n",
    "                n_mels=config[\"n_mels\"], \n",
    "                n_fft=config[\"n_fft\"]\n",
    "            )\n",
    "            log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "            \n",
    "            # Standardize spectrogram width\n",
    "            if log_spectrogram.shape[1] > config[\"max_spectrogram_width\"]:\n",
    "                log_spectrogram = log_spectrogram[:, :config[\"max_spectrogram_width\"]]\n",
    "            else:\n",
    "                pad_width = config[\"max_spectrogram_width\"] - log_spectrogram.shape[1]\n",
    "                log_spectrogram = np.pad(log_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "            spectrograms.append(log_spectrogram)\n",
    "    \n",
    "    else:\n",
    "        # --- Single Slice for Other Sounds ---\n",
    "        # Truncate or pad the audio to the window duration\n",
    "        if len(y) > window_samples:\n",
    "            y = y[:window_samples]\n",
    "        else:\n",
    "            y = np.pad(y, (0, window_samples - len(y)), 'constant')\n",
    "\n",
    "        # Generate a single Mel spectrogram\n",
    "        spectrogram = librosa.feature.melspectrogram(\n",
    "            y=y, \n",
    "            sr=config[\"sample_rate\"], \n",
    "            n_mels=config[\"n_mels\"], \n",
    "            n_fft=config[\"n_fft\"]\n",
    "        )\n",
    "        log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        \n",
    "        # Standardize spectrogram width (same logic as above)\n",
    "        if log_spectrogram.shape[1] > config[\"max_spectrogram_width\"]:\n",
    "            log_spectrogram = log_spectrogram[:, :config[\"max_spectrogram_width\"]]\n",
    "        else:\n",
    "            pad_width = config[\"max_spectrogram_width\"] - log_spectrogram.shape[1]\n",
    "            log_spectrogram = np.pad(log_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "        spectrograms.append(log_spectrogram)\n",
    "        \n",
    "    return spectrograms\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Loads all audio files, converts them, and creates labels.\"\"\"\n",
    "    X, y = [], []\n",
    "    class_map = {label: i for i, label in enumerate(os.listdir(data_path))}\n",
    "    \n",
    "    for label, class_idx in class_map.items():\n",
    "        class_dir = os.path.join(data_path, label)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                filepath = os.path.join(class_dir, filename)\n",
    "                spectrogram_list = process_audio_file(filepath, label, CONFIG)\n",
    "                \n",
    "                if spectrogram_list:\n",
    "                    # Add all spectrograms from the list to our dataset\n",
    "                    X.extend(spectrogram_list)\n",
    "                    # Add a label for each spectrogram that was generated\n",
    "                    y.extend([class_idx] * len(spectrogram_list))\n",
    "                    \n",
    "    return np.array(X), np.array(y), class_map\n",
    "\n",
    "# --- Execute Data Preparation ---\n",
    "X, y, class_map = load_data(DATA_PATH)\n",
    "print(\"Class mapping:\", class_map)\n",
    "\n",
    "# Add a channel dimension for the CNN\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=42, stratify=None)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122e18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9178 - loss: 0.3049 - val_accuracy: 0.9960 - val_loss: 0.0322\n",
      "Epoch 2/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9611 - loss: 0.0829 - val_accuracy: 0.9978 - val_loss: 0.0171\n",
      "Epoch 3/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9669 - loss: 0.0614 - val_accuracy: 0.9982 - val_loss: 0.0094\n",
      "Epoch 4/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9664 - loss: 0.0543 - val_accuracy: 0.9960 - val_loss: 0.0122\n",
      "Epoch 5/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9682 - loss: 0.0543 - val_accuracy: 0.9991 - val_loss: 0.0071\n",
      "Epoch 6/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9669 - loss: 0.0546 - val_accuracy: 0.9973 - val_loss: 0.0096\n",
      "Epoch 7/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9629 - loss: 0.0548 - val_accuracy: 0.9965 - val_loss: 0.0109\n",
      "Epoch 8/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9695 - loss: 0.0515 - val_accuracy: 0.9991 - val_loss: 0.0032\n",
      "Epoch 9/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9686 - loss: 0.0454 - val_accuracy: 0.9987 - val_loss: 0.0039\n",
      "Epoch 10/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9673 - loss: 0.0481 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "Epoch 11/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9691 - loss: 0.0404 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "Epoch 12/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9726 - loss: 0.0358 - val_accuracy: 0.9996 - val_loss: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9678 - loss: 0.0415 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9669 - loss: 0.0489 - val_accuracy: 0.9996 - val_loss: 8.0956e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9700 - loss: 0.0396 - val_accuracy: 0.9996 - val_loss: 8.1991e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9686 - loss: 0.0430 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
      "Epoch 17/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9691 - loss: 0.0414 - val_accuracy: 0.9996 - val_loss: 0.0016\n",
      "Epoch 18/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9585 - loss: 0.0483 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 19/30\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9638 - loss: 0.0492 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "71/71 - 0s - 5ms/step - accuracy: 0.9996 - loss: 8.0956e-04\n",
      "\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# --- Compile the Model ---\n",
    "model = build_cnn_model(input_shape=X_train.shape[1:], num_classes=len(class_map))\n",
    "\n",
    "# Using Adam optimizer and SparseCategoricalCrossentropy because our labels are integers.\n",
    "# If you one-hot encode your labels, use CategoricalCrossentropy.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Train the Model ---\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X, y),\n",
    "                    callbacks=[\n",
    "                        # Stop training early if validation loss stops improving\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "                    ])\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "test_loss, test_acc = model.evaluate(X, y, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97f80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp93q_93ft/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp93q_93ft/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp93q_93ft'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 48, 1), dtype=tf.float32, name='keras_tensor_55')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130522801495824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801495056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801501200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801495248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801501584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801501392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801501968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801501776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801502352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130522801502160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite model saved as lg_sound_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1754329050.159674  304256 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1754329050.159689  304256 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-08-04 13:37:30.159882: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp93q_93ft\n",
      "2025-08-04 13:37:30.160679: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-08-04 13:37:30.160691: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp93q_93ft\n",
      "2025-08-04 13:37:30.167648: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-08-04 13:37:30.207367: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp93q_93ft\n",
      "2025-08-04 13:37:30.219829: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 59949 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Create a representative dataset for quantization\n",
    "def representative_dataset():\n",
    "    for i in range(100): # Use a subset of the training data\n",
    "      # Ensure the data type is float32\n",
    "      yield [X_train[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.representative_dataset = representative_dataset\n",
    "# Ensure that if the ops are not supported by the TFLite runtime, the converter throws an error.\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (or int8)\n",
    "#converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "#converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# --- Save the Model ---\n",
    "with open('lg_sound_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"TFLite model saved as lg_sound_model.tflite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
