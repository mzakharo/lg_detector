{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de445766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:33:37.042408: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-30 00:33:37.047153: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-30 00:33:37.059174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753850017.081605   99829 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753850017.087669   99829 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753850017.104003   99829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753850017.104028   99829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753850017.104030   99829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753850017.104032   99829 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-30 00:33:40.509649: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m1,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │       \u001b[38;5;34m262,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,130</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m268,130\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,130</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,130\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a lightweight CNN model for audio classification.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input spectrograms (height, width, channels).\n",
    "        num_classes (int): The number of output classes (e.g., 2 for melody vs. other).\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input Layer\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        # First Convolutional Block\n",
    "        # Using smaller filters (3x3) and fewer of them (8) to keep the model small.\n",
    "        layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Flatten the feature map to feed into the dense layers\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Dense Layer for classification\n",
    "        # A smaller dense layer (32 units) to reduce parameters\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.5), # Dropout helps prevent overfitting\n",
    "\n",
    "        # Output Layer\n",
    "        # The number of units equals the number of classes.\n",
    "        # Use 'softmax' for multi-class or 'sigmoid' for binary classification.\n",
    "        layers.Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example of how to create the model\n",
    "# These values will be determined during data preprocessing\n",
    "INPUT_SHAPE = (128, 128, 1) # (n_mels, time_steps, channels)\n",
    "NUM_CLASSES = 2 # (lg_melody, other_sounds)\n",
    "\n",
    "model = build_cnn_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46aef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/karoldvl/ESC-50/archive/master.zip\n",
      "645701632/Unknown \u001b[1m40s\u001b[0m 0us/step"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/other_sounds/esc-50_extracted'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.get_file('esc-50.zip',\n",
    "                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='data/other_sounds',\n",
    "                        extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71e18f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'other_sounds': 0, 'lg_melody': 1}\n",
      "Training set shape: (1847, 64, 48, 1)\n",
      "Validation set shape: (103, 64, 48, 1)\n",
      "Test set shape: (103, 64, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"sample_rate\": 16000,    # Hz\n",
    "    \"window_duration\": 1.5,  # seconds (length of one spectrogram)\n",
    "    \"hop_duration\": 0.5,     # seconds (how much to slide the window)\n",
    "    \"n_mels\": 64,            # Number of Mel bands (reduced for ESP32 efficiency)\n",
    "    \"n_fft\": 1024,           # Number of FFT points\n",
    "    \"max_spectrogram_width\": 48 # Fixed width for spectrograms (time steps)\n",
    "}\n",
    "\n",
    "\n",
    "def process_audio_file(audio_path, class_label, config):\n",
    "    \"\"\"\n",
    "    Loads an audio file and converts it into one or more Mel spectrograms.\n",
    "\n",
    "    - If class_label is 'lg_melody', it uses a sliding window to generate\n",
    "      multiple, overlapping spectrograms from the entire clip.\n",
    "    - Otherwise, it generates a single spectrogram from the start of the clip.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        class_label (str): The name of the class (e.g., 'lg_melody').\n",
    "        config (dict): A dictionary of processing parameters.\n",
    "\n",
    "    Returns:\n",
    "        A list of spectrograms. Returns an empty list if processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=config[\"sample_rate\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    spectrograms = []\n",
    "    window_samples = int(config[\"window_duration\"] * config[\"sample_rate\"])\n",
    "    hop_samples = int(config[\"hop_duration\"] * config[\"sample_rate\"])\n",
    "\n",
    "    if class_label == 'lg_melody':\n",
    "        # --- Sliding Window for the Target Melody ---\n",
    "        for start in range(0, len(y) - window_samples, hop_samples):\n",
    "            end = start + window_samples\n",
    "            chunk = y[start:end]\n",
    "            \n",
    "            # Generate Mel spectrogram for the chunk\n",
    "            spectrogram = librosa.feature.melspectrogram(\n",
    "                y=chunk, \n",
    "                sr=sr, \n",
    "                n_mels=config[\"n_mels\"], \n",
    "                n_fft=config[\"n_fft\"]\n",
    "            )\n",
    "            log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "            \n",
    "            # Standardize spectrogram width\n",
    "            if log_spectrogram.shape[1] > config[\"max_spectrogram_width\"]:\n",
    "                log_spectrogram = log_spectrogram[:, :config[\"max_spectrogram_width\"]]\n",
    "            else:\n",
    "                pad_width = config[\"max_spectrogram_width\"] - log_spectrogram.shape[1]\n",
    "                log_spectrogram = np.pad(log_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "            spectrograms.append(log_spectrogram)\n",
    "    \n",
    "    else:\n",
    "        # --- Single Slice for Other Sounds ---\n",
    "        # Truncate or pad the audio to the window duration\n",
    "        if len(y) > window_samples:\n",
    "            y = y[:window_samples]\n",
    "        else:\n",
    "            y = np.pad(y, (0, window_samples - len(y)), 'constant')\n",
    "\n",
    "        # Generate a single Mel spectrogram\n",
    "        spectrogram = librosa.feature.melspectrogram(\n",
    "            y=y, \n",
    "            sr=config[\"sample_rate\"], \n",
    "            n_mels=config[\"n_mels\"], \n",
    "            n_fft=config[\"n_fft\"]\n",
    "        )\n",
    "        log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        \n",
    "        # Standardize spectrogram width (same logic as above)\n",
    "        if log_spectrogram.shape[1] > config[\"max_spectrogram_width\"]:\n",
    "            log_spectrogram = log_spectrogram[:, :config[\"max_spectrogram_width\"]]\n",
    "        else:\n",
    "            pad_width = config[\"max_spectrogram_width\"] - log_spectrogram.shape[1]\n",
    "            log_spectrogram = np.pad(log_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "        spectrograms.append(log_spectrogram)\n",
    "        \n",
    "    return spectrograms\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Loads all audio files, converts them, and creates labels.\"\"\"\n",
    "    X, y = [], []\n",
    "    class_map = {label: i for i, label in enumerate(os.listdir(data_path))}\n",
    "    \n",
    "    for label, class_idx in class_map.items():\n",
    "        class_dir = os.path.join(data_path, label)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                filepath = os.path.join(class_dir, filename)\n",
    "                spectrogram_list = process_audio_file(filepath, label, CONFIG)\n",
    "                \n",
    "                if spectrogram_list:\n",
    "                    # Add all spectrograms from the list to our dataset\n",
    "                    X.extend(spectrogram_list)\n",
    "                    # Add a label for each spectrogram that was generated\n",
    "                    y.extend([class_idx] * len(spectrogram_list))\n",
    "                    \n",
    "    return np.array(X), np.array(y), class_map\n",
    "\n",
    "# --- Execute Data Preparation ---\n",
    "X, y, class_map = load_data(DATA_PATH)\n",
    "print(\"Class mapping:\", class_map)\n",
    "\n",
    "# Add a channel dimension for the CNN\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=42, stratify=None)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122e18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9635 - loss: 0.3276 - val_accuracy: 0.9742 - val_loss: 0.1145\n",
      "Epoch 2/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0773 - val_accuracy: 0.9742 - val_loss: 0.0289\n",
      "Epoch 3/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0371 - val_accuracy: 0.9742 - val_loss: 0.0268\n",
      "Epoch 4/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0445 - val_accuracy: 0.9742 - val_loss: 0.0228\n",
      "Epoch 5/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0347 - val_accuracy: 0.9742 - val_loss: 0.0218\n",
      "Epoch 6/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0319 - val_accuracy: 0.9742 - val_loss: 0.0208\n",
      "Epoch 7/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9742 - loss: 0.0286 - val_accuracy: 0.9742 - val_loss: 0.0198\n",
      "Epoch 8/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9795 - loss: 0.0245 - val_accuracy: 0.9912 - val_loss: 0.0161\n",
      "Epoch 9/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9888 - loss: 0.0227 - val_accuracy: 0.9971 - val_loss: 0.0068\n",
      "Epoch 10/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9878 - loss: 0.0203 - val_accuracy: 0.9995 - val_loss: 0.0033\n",
      "Epoch 11/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9903 - loss: 0.0188 - val_accuracy: 0.9946 - val_loss: 0.0112\n",
      "Epoch 12/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9942 - loss: 0.0161 - val_accuracy: 0.9985 - val_loss: 0.0026\n",
      "Epoch 13/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9956 - loss: 0.0095 - val_accuracy: 0.9971 - val_loss: 0.0094\n",
      "Epoch 14/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9932 - loss: 0.0156 - val_accuracy: 0.9971 - val_loss: 0.0107\n",
      "Epoch 15/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0188 - val_accuracy: 1.0000 - val_loss: 0.0037\n",
      "Epoch 16/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9937 - loss: 0.0107 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 17/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9946 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 3.3081e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9922 - loss: 0.0219 - val_accuracy: 0.9981 - val_loss: 0.0039\n",
      "Epoch 19/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9849 - loss: 0.0282 - val_accuracy: 0.9961 - val_loss: 0.0073\n",
      "Epoch 20/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9907 - loss: 0.0175 - val_accuracy: 0.9942 - val_loss: 0.0041\n",
      "Epoch 21/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 1.1267e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9932 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.3777e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9907 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 4.7422e-05\n",
      "Epoch 24/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9927 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 8.6736e-06\n",
      "Epoch 25/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9912 - loss: 0.0018 - val_accuracy: 0.9995 - val_loss: 0.0011\n",
      "Epoch 26/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 1.4109e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9951 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 5.3360e-05\n",
      "Epoch 28/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9912 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 3.4823e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9791 - loss: 0.0731 - val_accuracy: 0.9830 - val_loss: 0.0774\n",
      "65/65 - 0s - 5ms/step - accuracy: 1.0000 - loss: 8.6736e-06\n",
      "\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# --- Compile the Model ---\n",
    "model = build_cnn_model(input_shape=X_train.shape[1:], num_classes=len(class_map))\n",
    "\n",
    "# Using Adam optimizer and SparseCategoricalCrossentropy because our labels are integers.\n",
    "# If you one-hot encode your labels, use CategoricalCrossentropy.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Train the Model ---\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X, y),\n",
    "                    callbacks=[\n",
    "                        # Stop training early if validation loss stops improving\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "                    ])\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "test_loss, test_acc = model.evaluate(X, y, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp31_i1bhf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp31_i1bhf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp31_i1bhf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 48, 1), dtype=tf.float32, name='keras_tensor_33')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130746471511248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471508560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471504720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471508944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471498768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471512592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471498000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471508368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471511440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130746471505296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized TFLite model saved as lg_sound_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1753853144.620440   99829 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1753853144.620474   99829 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-07-30 01:25:44.620841: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp31_i1bhf\n",
      "2025-07-30 01:25:44.621559: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-07-30 01:25:44.621571: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp31_i1bhf\n",
      "I0000 00:00:1753853144.627308   99829 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-07-30 01:25:44.628464: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-07-30 01:25:44.666378: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp31_i1bhf\n",
      "2025-07-30 01:25:44.678584: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 57747 microseconds.\n",
      "2025-07-30 01:25:44.695731: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "# Create a representative dataset for quantization\n",
    "def representative_dataset():\n",
    "    for i in range(100): # Use a subset of the training data\n",
    "      # Ensure the data type is float32\n",
    "      yield [X_train[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "# Ensure that if the ops are not supported by the TFLite runtime, the converter throws an error.\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (or int8)\n",
    "#converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "#converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# --- Save the Model ---\n",
    "with open('lg_sound_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"Quantized TFLite model saved as lg_sound_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1b6c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
